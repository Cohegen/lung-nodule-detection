{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Nodule Detection with 3D CNN (Improved)\n",
    "\n",
    "This notebook implements an improved 3D CNN for lung nodule detection with:\n",
    "- Data augmentation (random flips, Gaussian noise)\n",
    "- Deeper model with dropout\n",
    "- AdamW optimizer\n",
    "- CosineAnnealingWarmRestarts scheduler\n",
    "- Focal Loss for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (uncomment for cloud)\n",
    "# !pip install torch torchvision SimpleITK matplotlib diskcache tqdm numpy scikit-learn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    DATA_DIR = './synthetic_luna'\n",
    "    SCAN_DIM = 64\n",
    "    NUM_SCANS = 20\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 5\n",
    "    LEARNING_RATE = 1e-3\n",
    "    CONV_CHANNELS = 16  # Increased channels\n",
    "    CACHE_DIR = './cache'\n",
    "\n",
    "os.makedirs(Config.DATA_DIR, exist_ok=True)\n",
    "os.makedirs(Config.CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "def create_synthetic_luna_dataset():\n",
    "    subset_dir = os.path.join(Config.DATA_DIR, \"subset0\")\n",
    "    os.makedirs(subset_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(Config.NUM_SCANS):\n",
    "        scan_id = f\"scan_{i}\"\n",
    "        \n",
    "        # Create .mhd file\n",
    "        mhd_content = f\"\"\"ObjectType = Image\n",
    "NDims = 3\n",
    "BinaryData = True\n",
    "BinaryDataByteOrderMSB = False\n",
    "CompressedData = False\n",
    "TransformMatrix = 1 0 0 0 1 0 0 0 1\n",
    "Offset = 0 0 0\n",
    "CenterOfRotation = 0 0 0\n",
    "AnatomicalOrientation = RAI\n",
    "ElementSpacing = 1 1 1\n",
    "DimSize = {Config.SCAN_DIM} {Config.SCAN_DIM} {Config.SCAN_DIM}\n",
    "ElementType = MET_FLOAT\n",
    "ElementDataFile = {scan_id}.raw\n",
    "\"\"\"\n",
    "        \n",
    "        with open(os.path.join(subset_dir, f\"{scan_id}.mhd\"), 'w') as f:\n",
    "            f.write(mhd_content)\n",
    "        \n",
    "        # Create CT data with nodules\n",
    "        ct_data = np.random.normal(0, 100, (Config.SCAN_DIM, Config.SCAN_DIM, Config.SCAN_DIM)).astype(np.float32)\n",
    "        \n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            x = np.random.randint(10, Config.SCAN_DIM-10)\n",
    "            y = np.random.randint(10, Config.SCAN_DIM-10)\n",
    "            z = np.random.randint(10, Config.SCAN_DIM-10)\n",
    "            radius = np.random.randint(3, 8)\n",
    "            \n",
    "            for dx in range(-radius, radius+1):\n",
    "                for dy in range(-radius, radius+1):\n",
    "                    for dz in range(-radius, radius+1):\n",
    "                        if dx*dx + dy*dy + dz*dz <= radius*radius:\n",
    "                            if (0 <= x+dx < Config.SCAN_DIM and \n",
    "                                0 <= y+dy < Config.SCAN_DIM and \n",
    "                                0 <= z+dz < Config.SCAN_DIM):\n",
    "                                ct_data[x+dx, y+dy, z+dz] += np.random.normal(200, 50)\n",
    "        \n",
    "        ct_data.tofile(os.path.join(subset_dir, f\"{scan_id}.raw\"))\n",
    "    \n",
    "    # Create candidates.csv\n",
    "    with open(os.path.join(Config.DATA_DIR, \"candidates.csv\"), 'w') as f:\n",
    "        f.write(\"seriesuid,coordX,coordY,coordZ,class\\n\")\n",
    "        for i in range(Config.NUM_SCANS):\n",
    "            scan_id = f\"scan_{i}\"\n",
    "            for k in range(np.random.randint(2, 6)):\n",
    "                x = np.random.uniform(10, Config.SCAN_DIM-10)\n",
    "                y = np.random.uniform(10, Config.SCAN_DIM-10)\n",
    "                z = np.random.uniform(10, Config.SCAN_DIM-10)\n",
    "                is_nodule = np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "                f.write(f\"{scan_id},{x},{y},{z},{is_nodule}\\n\")\n",
    "\n",
    "create_synthetic_luna_dataset()\n",
    "print(\"Dataset created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation in dataset\n",
    "class SyntheticLunaDataset(Dataset):\n",
    "    def __init__(self, data_dir=Config.DATA_DIR, scan_dim=Config.SCAN_DIM, augment=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.scan_dim = scan_dim\n",
    "        self.samples = []\n",
    "        self.augment = augment\n",
    "        \n",
    "        candidates_path = os.path.join(data_dir, \"candidates.csv\")\n",
    "        with open(candidates_path, 'r') as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                self.samples.append({\n",
    "                    'series_uid': parts[0],\n",
    "                    'coord_x': float(parts[1]),\n",
    "                    'coord_y': float(parts[2]),\n",
    "                    'coord_z': float(parts[3]),\n",
    "                    'is_nodule': int(parts[4])\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        scan_id = sample['series_uid']\n",
    "        \n",
    "        raw_path = os.path.join(self.data_dir, \"subset0\", f\"{scan_id}.raw\")\n",
    "        ct = np.fromfile(raw_path, dtype=np.float32).reshape(self.scan_dim, self.scan_dim, self.scan_dim)\n",
    "        \n",
    "        ct = np.clip(ct, -1000, 1000)\n",
    "        ct = (ct - ct.min()) / (ct.max() - ct.min() + 1e-8)\n",
    "        \n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            for axis in range(3):\n",
    "                if np.random.rand() > 0.5:\n",
    "                    ct = np.flip(ct, axis=axis)\n",
    "            ct = ct + np.random.normal(0, 0.05, ct.shape)\n",
    "            ct = np.clip(ct, 0, 1)\n",
    "        \n",
    "        ct = torch.tensor(ct, dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor([1 - sample['is_nodule'], sample['is_nodule']], dtype=torch.float32)\n",
    "        \n",
    "        return ct, label, scan_id\n",
    "\n",
    "# Create datasets and loaders\n",
    "full_dataset = SyntheticLunaDataset()\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "indices = np.arange(len(full_dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(SyntheticLunaDataset(augment=True), train_indices)\n",
    "val_dataset = torch.utils.data.Subset(SyntheticLunaDataset(augment=False), val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved model with dropout and more channels\n",
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels, dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(conv_channels, conv_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(2, 2)\n",
    "        self.dropout = nn.Dropout3d(dropout_p)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        block_out = self.maxpool(block_out)\n",
    "        block_out = self.dropout(block_out)\n",
    "        return block_out\n",
    "\n",
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, conv_channels=Config.CONV_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
    "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
    "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
    "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
    "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
    "        \n",
    "        final_size = 4 * 4 * 4 * (conv_channels * 8)\n",
    "        self.head_linear = nn.Linear(final_size, 2)\n",
    "        self.head_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) in {nn.Linear, nn.Conv3d, nn.Conv2d, nn.ConvTranspose2d, nn.ConvTranspose3d}:\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.tail_batchnorm(input_batch)\n",
    "        block_out = self.block1(bn_output)\n",
    "        block_out = self.block2(block_out)\n",
    "        block_out = self.block3(block_out)\n",
    "        block_out = self.block4(block_out)\n",
    "        conv_flat = block_out.view(block_out.size(0), -1)\n",
    "        linear_output = self.head_linear(conv_flat)\n",
    "        return linear_output, self.head_softmax(linear_output)\n",
    "\n",
    "model = LunaModel().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with AdamW, CosineAnnealing, and Focal Loss\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(Config.NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for ct, labels, scan_ids in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS} [Train]\"):\n",
    "        ct, labels = ct.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(ct)\n",
    "        loss = criterion(outputs, labels.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_correct += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ct, labels, scan_ids in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS} [Val]\"):\n",
    "            ct, labels = ct.to(device), labels.to(device)\n",
    "            outputs, _ = model(ct)\n",
    "            loss = criterion(outputs, labels.argmax(dim=1))\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.3f}\")\n",
    "    print(f\"          Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.3f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax1.plot(val_losses, label='Validation Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(train_accuracies, label='Train Accuracy', marker='o')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.3f}\")\n",
    "print(f\"Best Validation Accuracy: {max(val_accuracies):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ct, labels, scan_ids in val_loader:\n",
    "        ct, labels = ct.to(device), labels.to(device)\n",
    "        outputs, probabilities = model(ct)\n",
    "        \n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        true_labels = labels.argmax(dim=1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(true_labels.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "auc = roc_auc_score(all_labels, all_probabilities)\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"AUC-ROC: {auc:.3f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.yticks([0, 1], ['Negative', 'Positive'])\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save improved model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': Config.NUM_EPOCHS,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'config': {\n",
    "        'scan_dim': Config.SCAN_DIM,\n",
    "        'conv_channels': Config.CONV_CHANNELS,\n",
    "        'batch_size': Config.BATCH_SIZE,\n",
    "        'learning_rate': Config.LEARNING_RATE\n",
    "    }\n",
    "}, 'lung_nodule_model_improved.pth')\n",
    "\n",
    "print(\"Improved model saved as 'lung_nodule_model_improved.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}